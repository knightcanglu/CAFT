import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import get_linear_schedule_with_warmup
import json
import os
import random
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
import torch.nn.functional as F
import numpy as np
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ==============================================================================
# 1. é…ç½®åŒºåŸŸï¼ˆæ–°å¢ç‰¹å¾è·¯å¾„å’Œåäº‹å®å‚æ•°ï¼‰
# ==============================================================================
class Config:
    mode = "baseline"     # cdanet or baseline
    clip_model_name = "openai/clip-vit-large-patch14-336"
    feat_dir = "./clip_large_features"  # é¢„æå–ç‰¹å¾è·¯å¾„

    # --- æ¨¡å‹æ¶æ„æ§åˆ¶å¼€å…³ ---
    use_multimodal_fusion = False
    use_gated_delta = True
    use_attention_pooling = True

    # --- æŸå¤±å‡½æ•°æ§åˆ¶å¼€å…³ ---
    use_consistency_loss = True
    use_contrastive_loss = True
    # æ–°å¢ï¼šåäº‹å®ç›¸å…³å‚æ•°
    use_strategy_weighting = False  # æ˜¯å¦æ ¹æ®åäº‹å®ç”Ÿæˆç­–ç•¥è¿›è¡ŒåŠ æƒ
    
    # --- è®¾å¤‡å’Œè¶…å‚æ•° ---
    device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
    batch_size = 16 
    num_epochs = 30
    head_lr = 5e-6
    weight_decay = 0.05
    max_grad_norm = 1.0
    num_warmup_steps = 1000
    lambda_consistency = 20
    lambda_contrastive = 0.5
    contrastive_temperature = 0.07

# ==============================================================================
# 2. æ•°æ®é›†å®šä¹‰ï¼ˆé€‚é…åäº‹å®æ ·æœ¬ç»“æ„ï¼‰
# ==============================================================================
class CausalPairFeatureDataset(Dataset):
    """åŠ è½½é¢„æå–ç‰¹å¾çš„CausalPairæ•°æ®é›†ï¼Œé€‚é…åäº‹å®æ ·æœ¬ç»“æ„"""
    def __init__(self, feat_dir, split="train_cda"):
        self.feat_dir = feat_dir
        # åŠ è½½æ‰€æœ‰ç‰¹å¾
        self.img_pool = np.load(os.path.join(feat_dir, f"{split}_img_pool.npy"))
        self.anchor_text_pool = np.load(os.path.join(feat_dir, f"{split}_anchor_text_pool.npy"))
        self.anchor_text_seq = np.load(os.path.join(feat_dir, f"{split}_anchor_text_seq.npy"))
        self.positive_text_pool = np.load(os.path.join(feat_dir, f"{split}_positive_text_pool.npy"))
        self.positive_text_seq = np.load(os.path.join(feat_dir, f"{split}_positive_text_seq.npy"))
        self.hardneg_text_pool = np.load(os.path.join(feat_dir, f"{split}_hardneg_text_pool.npy"))
        self.hardneg_text_seq = np.load(os.path.join(feat_dir, f"{split}_hardneg_text_seq.npy"))
        self.labels = np.load(os.path.join(feat_dir, f"{split}_labels.npy"))
        
        # å¦‚æœå­˜åœ¨ç­–ç•¥ä¿¡æ¯ï¼ŒåŠ è½½ç­–ç•¥æƒé‡ï¼ˆç”¨äºåŠ æƒæŸå¤±ï¼‰
        strategy_weights_path = os.path.join(feat_dir, f"{split}_strategy_weights.npy")
        if os.path.exists(strategy_weights_path):
            self.strategy_weights = np.load(strategy_weights_path)
        else:
            self.strategy_weights = np.ones(len(self.labels))  # é»˜è®¤æƒé‡ä¸º1

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "img_pool": self.img_pool[idx],
            "anchor_text_pool": self.anchor_text_pool[idx],
            "anchor_text_seq": self.anchor_text_seq[idx],
            "positive_text_pool": self.positive_text_pool[idx],
            "positive_text_seq": self.positive_text_seq[idx],
            "hardneg_text_pool": self.hardneg_text_pool[idx],
            "hardneg_text_seq": self.hardneg_text_seq[idx],
            "label": torch.tensor(self.labels[idx], dtype=torch.long),
            "strategy_weight": torch.tensor(self.strategy_weights[idx], dtype=torch.float32)
        }


class StandardFeatureDataset(Dataset):
    """åŠ è½½é¢„æå–ç‰¹å¾çš„æ ‡å‡†æ•°æ®é›†"""
    def __init__(self, feat_dir, split):
        self.feat_dir = feat_dir
        self.img_pool = np.load(os.path.join(feat_dir, f"{split}_img_pool.npy"))
        self.text_pool = np.load(os.path.join(feat_dir, f"{split}_text_pool.npy"))
        self.text_seq = np.load(os.path.join(feat_dir, f"{split}_text_seq.npy"))
        self.labels = np.load(os.path.join(feat_dir, f"{split}_labels.npy"))

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "img_pool": self.img_pool[idx],
            "text_pool": self.text_pool[idx],
            "text_seq": self.text_seq[idx],
            "label": torch.tensor(self.labels[idx], dtype=torch.long)
        }

def custom_collate_fn(batch):
    """å°†ç‰¹å¾è½¬æ¢ä¸ºtensorå¹¶æ•´ç†æˆbatchï¼Œå¢åŠ ç­–ç•¥æƒé‡å¤„ç†"""
    collated = {
        "img_pool": torch.tensor(np.stack([x["img_pool"] for x in batch]), dtype=torch.float32),
        "anchor_text_pool": torch.tensor(np.stack([x["anchor_text_pool"] for x in batch]), dtype=torch.float32) if "anchor_text_pool" in batch[0] else None,
        "anchor_text_seq": torch.tensor(np.stack([x["anchor_text_seq"] for x in batch]), dtype=torch.float32) if "anchor_text_seq" in batch[0] else None,
        "positive_text_pool": torch.tensor(np.stack([x["positive_text_pool"] for x in batch]), dtype=torch.float32) if "positive_text_pool" in batch[0] else None,
        "positive_text_seq": torch.tensor(np.stack([x["positive_text_seq"] for x in batch]), dtype=torch.float32) if "positive_text_seq" in batch[0] else None,
        "hardneg_text_pool": torch.tensor(np.stack([x["hardneg_text_pool"] for x in batch]), dtype=torch.float32) if "hardneg_text_pool" in batch[0] else None,
        "hardneg_text_seq": torch.tensor(np.stack([x["hardneg_text_seq"] for x in batch]), dtype=torch.float32) if "hardneg_text_seq" in batch[0] else None,
        "text_pool": torch.tensor(np.stack([x["text_pool"] for x in batch]), dtype=torch.float32) if "text_pool" in batch[0] else None,
        "text_seq": torch.tensor(np.stack([x["text_seq"] for x in batch]), dtype=torch.float32) if "text_seq" in batch[0] else None,
        "label": torch.stack([x["label"] for x in batch])
    }
    
    # æ·»åŠ ç­–ç•¥æƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "strategy_weight" in batch[0]:
        collated["strategy_weight"] = torch.stack([x["strategy_weight"] for x in batch])
        
    return collated

# ==============================================================================
# 3. æŸå¤±å‡½æ•°å®šä¹‰ï¼ˆå¢åŠ ç­–ç•¥åŠ æƒæ”¯æŒï¼‰
# ==============================================================================
class CausalConsistencyLoss(nn.Module):
    def __init__(self, use_strategy_weighting=False):
        super().__init__()
        self.use_strategy_weighting = use_strategy_weighting
        
    def forward(self, feat_anchor, feat_positive, weights=None):
        cos_sim = F.cosine_similarity(feat_anchor, feat_positive)
        loss = 1 - cos_sim
        
        if self.use_strategy_weighting and weights is not None:
            loss = loss * weights
        
        return torch.mean(loss)

class SupervisedContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.07, use_strategy_weighting=False):
        super().__init__()
        self.temperature = temperature
        self.use_strategy_weighting = use_strategy_weighting
        
    def forward(self, features, labels, weights=None):
        batch_size = features.shape[0]
        labels = labels.contiguous().view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(features.device)
        
        anchor_dot_contrast = torch.div(torch.matmul(features, features.T), self.temperature)
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()

        mask = mask.repeat(1, 1)
        logits_mask = torch.scatter(torch.ones_like(mask), 1, torch.arange(batch_size).view(-1, 1).to(features.device), 0)
        mask = mask * logits_mask
        
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))

        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)
        loss = -mean_log_prob_pos
        
        if self.use_strategy_weighting and weights is not None:
            loss = loss * weights
        
        return loss.mean()

# ==============================================================================
# 4. æ¨¡å‹æ¶æ„å®šä¹‰ï¼ˆä¿æŒä¸å˜ï¼Œå·²é€‚é…é¢„æå–ç‰¹å¾ï¼‰
# ==============================================================================
class CausalDeltaAttentionNet(nn.Module):
    def __init__(self,
                 clip_vision_dim=1024,  # CLIP-Largeè§†è§‰ç‰¹å¾ç»´åº¦
                 clip_text_dim=768,    # CLIP-Largeæ–‡æœ¬ç‰¹å¾ç»´åº¦
                 dropout_rate=0.4,
                 use_multimodal_fusion=True,
                 use_gated_delta=True,
                 use_attention_pooling=True):
        super().__init__()
        self.use_multimodal_fusion = use_multimodal_fusion
        self.use_gated_delta = use_gated_delta
        self.use_attention_pooling = use_attention_pooling

        # ç‰¹å¾ç»´åº¦
        self.clip_vision_dim = clip_vision_dim
        self.clip_text_dim = clip_text_dim

        # åˆ†ç±»å™¨è¾“å…¥ç»´åº¦ï¼ˆè§†è§‰+æ–‡æœ¬+å› æœç‰¹å¾ï¼‰
        new_classifier_input_dim = clip_vision_dim + clip_text_dim + clip_text_dim

        # è·¨æ¨¡æ€èåˆï¼ˆå›¾åƒåºåˆ— -> æ–‡æœ¬åºåˆ—ï¼‰
        if self.use_multimodal_fusion:
            self.image_text_cross_attention = nn.MultiheadAttention(
                embed_dim=clip_text_dim,
                kdim=clip_vision_dim,
                vdim=clip_vision_dim,
                num_heads=8,
                batch_first=True
            )
            self.fusion_norm = nn.LayerNorm(clip_text_dim)

        # é—¨æ§ç½‘ç»œï¼ˆç”¨äºtokençº§deltaï¼‰
        if self.use_gated_delta:
            self.delta_fusion_gate = nn.Sequential(
                nn.Linear(clip_text_dim * 2, clip_text_dim),
                nn.Sigmoid()
            )

        # CDAå¤´ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰
        self.cda_head = nn.MultiheadAttention(
            embed_dim=clip_text_dim,
            num_heads=8,
            batch_first=True
        )

        # æ³¨æ„åŠ›æ± åŒ–
        if self.use_attention_pooling:
            self.pooling_query = nn.Parameter(torch.randn(1, 1, clip_text_dim))
            self.attention_pooling = nn.MultiheadAttention(
                embed_dim=clip_text_dim,
                num_heads=8,
                batch_first=True
            )

        # åˆ†ç±»å™¨å’ŒæŠ•å½±å¤´
        self.classifier = nn.Sequential(
            nn.Linear(new_classifier_input_dim, 512),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 2)
        )
        self.proj_head = nn.Sequential(
            nn.Linear(new_classifier_input_dim, new_classifier_input_dim // 2),
            nn.ReLU(),
            nn.Linear(new_classifier_input_dim // 2, 128)
        )

    def encode_sequence(self, img_pool, text_pool, text_seq):
        """ç›´æ¥ä½¿ç”¨é¢„æå–çš„ç‰¹å¾ï¼ˆæ›¿ä»£CLIPç¼–ç ï¼‰"""
        # å…¨å±€ç‰¹å¾æ‹¼æ¥ (B, d_v + d_t)
        global_features = torch.cat((img_pool, text_pool), dim=1)
        # æ–‡æœ¬åºåˆ—ç‰¹å¾ (B, L_t, d_s)
        text_seq_features = text_seq
        # å›¾åƒåºåˆ—ç‰¹å¾ï¼ˆç®€åŒ–ä¸ºNoneï¼‰
        image_seq_features = None
        return global_features, text_seq_features, image_seq_features

    def get_causal_features(self, img_pool, anchor_text_pool, anchor_text_seq, cf_text_pool, cf_text_seq, return_explain=False):
        """ä½¿ç”¨é¢„æå–ç‰¹å¾è®¡ç®—å› æœç‰¹å¾"""
        # ç¼–ç é”šç‚¹å’Œåäº‹å®ç‰¹å¾
        z_a, S_a, V_a = self.encode_sequence(img_pool, anchor_text_pool, anchor_text_seq)
        z_c, S_c, V_c = self.encode_sequence(img_pool, cf_text_pool, cf_text_seq)

        # å…¨å±€delta
        final_delta = z_a - z_c
        seq_delta = None
        cda_weights = None
        pool_weights = None

        # å¤šæ¨¡æ€èåˆï¼ˆæ–‡æœ¬åºåˆ— + å›¾åƒåºåˆ—ï¼‰
        S_a_fused = S_a
        S_c_fused = S_c
        if self.use_multimodal_fusion and (V_a is not None):
            try:
                attn_out, _ = self.image_text_cross_attention(query=S_a, key=V_a, value=V_a, need_weights=False)
                S_a_fused = self.fusion_norm(S_a + attn_out)
                attn_out_c, _ = self.image_text_cross_attention(query=S_c, key=V_a, value=V_a, need_weights=False)
                S_c_fused = self.fusion_norm(S_c + attn_out_c)
            except:
                pass

        # è®¡ç®—åºåˆ—delta
        if S_a_fused is not None and S_c_fused is not None:
            seq_delta = S_a_fused - S_c_fused

            # é—¨æ§æœºåˆ¶
            if self.use_gated_delta:
                gate_in = torch.cat([S_a_fused, S_c_fused], dim=-1)
                gate_vals = self.delta_fusion_gate(gate_in)
                seq_delta = gate_vals * seq_delta

            # CDAå¤´æ³¨æ„åŠ›
            try:
                causal_seq, cda_weights = self.cda_head(query=S_a_fused, key=seq_delta, value=seq_delta, need_weights=True)
            except TypeError:
                causal_seq, cda_weights = self.cda_head(S_a_fused, seq_delta, seq_delta, need_weights=True)

            # æ± åŒ–
            if self.use_attention_pooling:
                bsz = causal_seq.size(0)
                pool_q = self.pooling_query.expand(bsz, -1, -1)
                pooled_causal, pool_weights = self.attention_pooling(query=pool_q, key=causal_seq, value=causal_seq, need_weights=True)
                pooled_causal = pooled_causal.squeeze(1)
            else:
                pooled_causal = causal_seq[:, 0, :]
        else:
            pooled_causal = torch.zeros(z_a.size(0), self.clip_text_dim, device=z_a.device)

        # èåˆç‰¹å¾
        try:
            pooled_causal_norm = self.fusion_norm(pooled_causal) if hasattr(self, "fusion_norm") else pooled_causal
        except:
            pooled_causal_norm = pooled_causal
        fused = torch.cat([z_a, pooled_causal_norm], dim=-1)
        features = fused

        if not return_explain:
            return features

        explain = {
            "final_delta": final_delta.detach().cpu().numpy() if final_delta is not None else None,
            "seq_delta": seq_delta.detach().cpu().numpy() if seq_delta is not None else None,
            "cda_weights": cda_weights.detach().cpu().numpy() if (cda_weights is not None and isinstance(cda_weights, torch.Tensor)) else None,
            "pool_weights": pool_weights.detach().cpu().numpy() if (pool_weights is not None and isinstance(pool_weights, torch.Tensor)) else None
        }
        return features, explain

    def forward(self, batch, return_explain=False):
        """å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨é¢„æå–ç‰¹å¾ï¼‰"""
        # ä»batchä¸­è·å–ç‰¹å¾
        img_pool = batch["img_pool"].to(self.classifier[0].weight.device)
        anchor_text_pool = batch["anchor_text_pool"].to(self.classifier[0].weight.device)
        anchor_text_seq = batch["anchor_text_seq"].to(self.classifier[0].weight.device)
        positive_text_pool = batch["positive_text_pool"].to(self.classifier[0].weight.device)
        positive_text_seq = batch["positive_text_seq"].to(self.classifier[0].weight.device)
        hardneg_text_pool = batch["hardneg_text_pool"].to(self.classifier[0].weight.device)
        hardneg_text_seq = batch["hardneg_text_seq"].to(self.classifier[0].weight.device)

        # è®¡ç®—é”šç‚¹ç‰¹å¾
        if return_explain:
            feat_anchor, explain_anchor = self.get_causal_features(
                img_pool, anchor_text_pool, anchor_text_seq,
                hardneg_text_pool, hardneg_text_seq, return_explain=True
            )
        else:
            feat_anchor = self.get_causal_features(
                img_pool, anchor_text_pool, anchor_text_seq,
                hardneg_text_pool, hardneg_text_seq, return_explain=False
            )
            explain_anchor = None

        # è®¡ç®—positiveå’Œhard-negativeç‰¹å¾
        feat_positive = self.get_causal_features(
            img_pool, anchor_text_pool, anchor_text_seq,
            positive_text_pool, positive_text_seq, return_explain=False
        )
        feat_hard_negative = self.get_causal_features(
            img_pool, anchor_text_pool, anchor_text_seq,
            hardneg_text_pool, hardneg_text_seq, return_explain=False
        )

        # åˆ†ç±»å’ŒæŠ•å½±
        logits = self.classifier(feat_anchor)
        proj_anchor = F.normalize(self.proj_head(feat_anchor), p=2, dim=1) if hasattr(self, "proj_head") else None
        proj_positive = F.normalize(self.proj_head(feat_positive), p=2, dim=1) if hasattr(self, "proj_head") else None
        proj_hardneg = F.normalize(self.proj_head(feat_hard_negative), p=2, dim=1) if hasattr(self, "proj_head") else None

        out = {
            "logits": logits,
            "feat_anchor": feat_anchor,
            "feat_positive": feat_positive,
            "feat_hard_negative": feat_hard_negative,
            "proj_anchor": proj_anchor,
            "proj_positive": proj_positive,
            "proj_hardneg": proj_hardneg
        }
        if return_explain:
            out["explain_anchor"] = explain_anchor
        return out


class BaselineCLIPClassifier(nn.Module):
    def __init__(self, clip_vision_dim=1024, clip_text_dim=768, dropout_rate=0.4):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(clip_vision_dim + clip_text_dim, 512),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 2)
        )

    def forward(self, batch):
        img_pool = batch["img_pool"].to(self.classifier[0].weight.device)
        text_pool = batch["text_pool"].to(self.classifier[0].weight.device)
        fused = torch.cat((img_pool, text_pool), dim=1)
        return self.classifier(fused)

# ==============================================================================
# 5. è®­ç»ƒä¸è¯„ä¼°å‡½æ•°ï¼ˆé€‚é…åäº‹å®åŠ æƒæŸå¤±ï¼‰
# ==============================================================================
def train_epoch_cda(model, dataloader, optimizer, scheduler, criterions, cfg):
    model.train()
    total_loss = 0
    total_cls_loss = 0
    total_cons_loss = 0
    total_cont_loss = 0

    pbar = tqdm(dataloader, desc="Training CDA-Net")
    for batch in pbar:
        labels = batch['label'].to(cfg.device)
        # è·å–ç­–ç•¥æƒé‡ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        strategy_weights = batch.get('strategy_weight', None)
        if strategy_weights is not None:
            strategy_weights = strategy_weights.to(cfg.device)
            
        optimizer.zero_grad()
        
        outputs = model(batch)
        
        feat_anchor = F.normalize(outputs['feat_anchor'], p=2, dim=1)
        feat_positive = F.normalize(outputs['feat_positive'], p=2, dim=1)
        feat_hard_negative = F.normalize(outputs['feat_hard_negative'], p=2, dim=1)
        
        loss_cls = criterions['cls'](outputs['logits'], labels)
        
        total_loss_batch = loss_cls
        total_cls_loss += loss_cls.item()
        
        loss_cons_val = 0
        loss_cont_val = 0

        if cfg.use_consistency_loss:
            # ä¼ å…¥ç­–ç•¥æƒé‡
            loss_cons = criterions['cons'](feat_anchor, feat_positive, strategy_weights)
            total_loss_batch += cfg.lambda_consistency * loss_cons
            loss_cons_val = loss_cons.item()
            total_cons_loss += loss_cons_val

        if cfg.use_contrastive_loss:
            features = torch.cat([feat_anchor, feat_positive, feat_hard_negative], dim=0)
            b_labels = torch.cat([labels, labels, labels], dim=0)
            
            # æ‰©å±•æƒé‡ä»¥åŒ¹é…æ‹¼æ¥åçš„ç‰¹å¾
            if strategy_weights is not None:
                b_weights = torch.cat([strategy_weights, strategy_weights, strategy_weights], dim=0)
            else:
                b_weights = None
                
            loss_cont = criterions['cont'](features, b_labels, b_weights)
            total_loss_batch += cfg.lambda_contrastive * loss_cont
            loss_cont_val = loss_cont.item()
            total_cont_loss += loss_cont_val

        total_loss_batch.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)
        optimizer.step()
        scheduler.step()
        
        total_loss += total_loss_batch.item()

        pbar.set_postfix({
            'Total': f"{total_loss_batch.item():.2f}",
            'CLS': f"{loss_cls.item():.2f}",
            'CONS': f"{loss_cons_val:.2f}",
            'CONT': f"{loss_cont_val:.2f}"
        })

    num_batches = len(dataloader)
    print(f"Avg CDA Training Loss: {total_loss / num_batches:.4f} | "
          f"Avg CLS: {total_cls_loss / num_batches:.4f} | "
          f"Avg CONS: {total_cons_loss / num_batches:.4f} | "
          f"Avg CONT: {total_cont_loss / num_batches:.4f}")

def train_epoch_baseline(model, dataloader, optimizer, scheduler, criterion, cfg):
    model.train(); total_loss = 0
    for batch in tqdm(dataloader, desc="Training Baseline"):
        labels = batch['label'].to(cfg.device); optimizer.zero_grad()
        logits = model(batch); loss = criterion(logits, labels); loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm); optimizer.step(); scheduler.step()
        total_loss += loss.item()
    print(f"Avg Baseline Training Loss: {total_loss / len(dataloader):.4f}")

def evaluate(model, dataloader, cfg, is_baseline=False):
    model.eval(); all_labels, all_preds, all_probs = [], [], []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            labels = batch['label']
            if is_baseline:
                logits = model(batch)
            else:
                # å¯¹éªŒè¯é›†ä½¿ç”¨æ–‡æœ¬è‡ªèº«ä½œä¸ºåäº‹å®ï¼ˆä¸è®­ç»ƒé€»è¾‘ä¸€è‡´ï¼‰
                logits = model.classifier(model.get_causal_features(
                    batch["img_pool"].to(cfg.device),
                    batch["text_pool"].to(cfg.device),
                    batch["text_seq"].to(cfg.device),
                    batch["text_pool"].to(cfg.device),
                    batch["text_seq"].to(cfg.device)
                ))
            probs = F.softmax(logits, dim=1)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(logits.argmax(dim=1).cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    auroc = roc_auc_score(all_labels, [p[1] for p in all_probs])
    return acc, f1, auroc


def set_seed(seed):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False

# ==============================================================================
# 6. ä¸»å‡½æ•°ï¼ˆé€‚é…æ–°æ•°æ®é›†å’Œæ¨¡å‹ï¼‰
# ==============================================================================
if __name__ == "__main__":
    cfg = Config()
    set_seed(2025)
    
    os.makedirs("checkpoints", exist_ok=True)
    BEST_MODEL_PATH = f"./checkpoints/best_model_{cfg.mode}_clip_large.pth"
    
    print(f"--- Running in mode: {cfg.mode.upper()} ---")
    if cfg.mode == "cdanet":
        print("CDA-Net Arch Config:", f"MMF={cfg.use_multimodal_fusion}", f"GD={cfg.use_gated_delta}", f"AP={cfg.use_attention_pooling}")
        print("CDA-Net Loss Config:", f"Consistency={cfg.use_consistency_loss}", f"Contrastive={cfg.use_contrastive_loss}",
              f"StrategyWeighting={cfg.use_strategy_weighting}")
        model = CausalDeltaAttentionNet(
            clip_vision_dim=1024,  # CLIP-Largeç»´åº¦
            clip_text_dim=768,
            use_multimodal_fusion=cfg.use_multimodal_fusion,
            use_gated_delta=cfg.use_gated_delta,
            use_attention_pooling=cfg.use_attention_pooling
        ).to(cfg.device)
        # åŠ è½½CDAè®­ç»ƒé›†ï¼ˆé¢„æå–ç‰¹å¾ï¼‰
        train_dataset = CausalPairFeatureDataset(cfg.feat_dir, split="train_cda")
        train_fn = train_epoch_cda
        
        criterions = {
            'cls': nn.CrossEntropyLoss(),
            'cons': CausalConsistencyLoss(use_strategy_weighting=cfg.use_strategy_weighting) if cfg.use_consistency_loss else None,
            'cont': SupervisedContrastiveLoss(
                temperature=cfg.contrastive_temperature,
                use_strategy_weighting=cfg.use_strategy_weighting
            ) if cfg.use_contrastive_loss else None
        }
        
        # ä»…è®­ç»ƒä¸‹æ¸¸å±‚ï¼ˆæ— CLIPå‚æ•°ï¼‰
        head_params = list(model.classifier.parameters()) + list(model.cda_head.parameters())
        if cfg.use_multimodal_fusion:
            head_params += list(model.image_text_cross_attention.parameters()) + list(model.fusion_norm.parameters())
        if cfg.use_gated_delta:
            head_params += list(model.delta_fusion_gate.parameters())
        if cfg.use_attention_pooling:
            head_params += list(model.attention_pooling.parameters()) + [model.pooling_query]
        optimizer = optim.AdamW(head_params, lr=cfg.head_lr, weight_decay=cfg.weight_decay)
    
    elif cfg.mode == "baseline":
        model = BaselineCLIPClassifier(clip_vision_dim=1024, clip_text_dim=768).to(cfg.device)
        train_dataset = StandardFeatureDataset(cfg.feat_dir, split="train_baseline")
        train_fn = train_epoch_baseline
        criterions = {'cls': nn.CrossEntropyLoss()}
        optimizer = optim.AdamW(model.parameters(), lr=cfg.head_lr, weight_decay=cfg.weight_decay)
    else:
        raise ValueError("Config.mode must be 'cdanet' or 'baseline'")

    # éªŒè¯é›†
    dev_dataset = StandardFeatureDataset(cfg.feat_dir, split="dev")
    train_dataloader = DataLoader(
        train_dataset, batch_size=cfg.batch_size, shuffle=True, 
        num_workers=4, collate_fn=custom_collate_fn
    )
    dev_dataloader = DataLoader(
        dev_dataset, batch_size=cfg.batch_size, shuffle=False, 
        num_workers=4, collate_fn=custom_collate_fn
    )
    
    total_steps = len(train_dataloader) * cfg.num_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=cfg.num_warmup_steps, 
        num_training_steps=total_steps
    )

    best_metric = 0.0  # ä»¥ACCä¸ºæŒ‡æ ‡

    print(f"Model: {model.__class__.__name__}, Feature dir: {cfg.feat_dir}")
    
    for epoch in range(1, cfg.num_epochs + 1):
        print(f"\n--- Epoch {epoch}/{cfg.num_epochs} ---")
        if cfg.mode == "cdanet":
            train_fn(model, train_dataloader, optimizer, scheduler, criterions, cfg)
        else:
            train_fn(model, train_dataloader, optimizer, scheduler, criterions['cls'], cfg)
        
        val_acc, val_f1, val_auc = evaluate(model, dev_dataloader, cfg, is_baseline=(cfg.mode=="baseline"))
        print(f"Epoch {epoch} Results: Val Acc = {val_acc:.4f}, Val Macro-F1 = {val_f1:.4f}, Val AUC = {val_auc:.4f}")

        if val_acc > best_metric:
            best_metric = val_acc
            print(f"ğŸ‰ New best model found! ACC: {best_metric:.4f}. Saving to {BEST_MODEL_PATH}")
            torch.save(model.state_dict(), BEST_MODEL_PATH)
        
    print("\n--- Training Complete ---")
    print(f"ğŸ† Best validation ACC: {best_metric:.4f}")
    print(f"Best model saved at: {BEST_MODEL_PATH}")